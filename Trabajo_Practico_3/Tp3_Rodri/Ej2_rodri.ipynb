{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generador basado en ejemplo del curso CS231 de Stanford: \n",
    "# CS231n Convolutional Neural Networks for Visual Recognition\n",
    "# (https://cs231n.github.io/neural-networks-case-study/)\n",
    "def generar_datos_clasificacion(cantidad_ejemplos, cantidad_clases):\n",
    "    FACTOR_ANGULO = 0.79\n",
    "    AMPLITUD_ALEATORIEDAD = 0.1\n",
    "\n",
    "    # Calculamos la cantidad de puntos por cada clase, asumiendo la misma cantidad para cada \n",
    "    # una (clases balanceadas)\n",
    "    n = int(cantidad_ejemplos / cantidad_clases)\n",
    "\n",
    "    # Entradas: 2 columnas (x1 y x2)\n",
    "    x = np.zeros((cantidad_ejemplos, 2))\n",
    "    \n",
    "    # Salida deseada (\"target\"): 1 columna que contendra la clase correspondiente (codificada como un entero)\n",
    "    t = np.zeros(cantidad_ejemplos, dtype=\"uint8\")  # 1 columna: la clase correspondiente (t -> \"target\")\n",
    "    \n",
    "    randomgen = np.random.default_rng()\n",
    "\n",
    "    # Por cada clase (que va de 0 a cantidad_clases)...\n",
    "    for clase in range(cantidad_clases):\n",
    "        # Tomando la ecuacion parametrica del circulo (x = r * cos(t), y = r * sin(t)), generamos \n",
    "        # radios distribuidos uniformemente entre 0 y 1 para la clase actual, y agregamos un poco de\n",
    "        # aleatoriedad\n",
    "        radios = np.linspace(0, 1, n) + AMPLITUD_ALEATORIEDAD * randomgen.standard_normal(size=n)\n",
    "\n",
    "        # ... y angulos distribuidos tambien uniformemente, con un desfasaje por cada clase\n",
    "        angulos = np.linspace(clase * np.pi * FACTOR_ANGULO, (clase + 1) * np.pi * FACTOR_ANGULO, n)\n",
    "\n",
    "        # Generamos un rango con los subindices de cada punto de esta clase. Este rango se va\n",
    "        # desplazando para cada clase: para la primera clase los indices estan en [0, n-1], para\n",
    "        # la segunda clase estan en [n, (2 * n) - 1], etc.\n",
    "        indices = range(clase * n, (clase + 1) * n)\n",
    "\n",
    "        # Generamos las \"entradas\", los valores de las variables independientes. Las variables:\n",
    "        # radios, angulos e indices tienen n elementos cada una, por lo que le estamos agregando\n",
    "        # tambien n elementos a la variable x (que incorpora ambas entradas, x1 y x2)\n",
    "        x1 = radios * np.sin(angulos)\n",
    "        x2 = radios * np.cos(angulos)\n",
    "        x[indices] = np.c_[x1, x2] #Esta función une al vector x1 y a x2. Es como que genera una matriz.\n",
    "\n",
    "        # Guardamos el valor de la clase que le vamos a asociar a las entradas x1 y x2 que acabamos\n",
    "        # de generar\n",
    "        t[indices] = clase\n",
    "\n",
    "    return x, t\n",
    "\n",
    "\n",
    "def inicializar_pesos(n_entrada, n_capa_2, n_capa_3):\n",
    "    randomgen = np.random.default_rng()\n",
    "\n",
    "    #w son los pesos, b son los sesgos. En este caso tenemos 2 capas luego de la capa de entrada\n",
    "    \n",
    "    #Se inicializan con un valor bajo. Por eso se multiplilcan por 0.1\n",
    "    w1 = 0.1 * randomgen.standard_normal((n_entrada, n_capa_2))\n",
    "    b1 = 0.1 * randomgen.standard_normal((1, n_capa_2))\n",
    "\n",
    "    w2 = 0.1 * randomgen.standard_normal((n_capa_2, n_capa_3))\n",
    "    b2 = 0.1 * randomgen.standard_normal((1,n_capa_3))\n",
    "\n",
    "    return {\"w1\": w1, \"b1\": b1, \"w2\": w2, \"b2\": b2}\n",
    "\n",
    "\n",
    "def ejecutar_adelante(x, pesos):\n",
    "    # Funcion de entrada (a.k.a. \"regla de propagacion\") para la primera capa oculta\n",
    "    z = x.dot(pesos[\"w1\"]) + pesos[\"b1\"]\n",
    "    #Por más que b1 sea un vector y la multiplicación de x*w sea una matriz, como tienen la misma cantidad de columnas, se puede hacer la suma porque numpy intuye que queremos sumar a cada fila de la matriz el mismo vector b.\n",
    "\n",
    "    # Funcion de activacion ReLU para la capa oculta (h -> \"hidden\")\n",
    "    h = np.maximum(0, z)\n",
    "    #Esta función de activación convierte a 0 los valores negativos, y deja como está a los valores positivos.\n",
    "\n",
    "    # Salida de la red (funcion de activacion lineal). Esto incluye la salida de todas\n",
    "    # las neuronas y para todos los ejemplos proporcionados\n",
    "    y = h.dot(pesos[\"w2\"]) + pesos[\"b2\"]\n",
    "\n",
    "    return {\"z\": z, \"h\": h, \"y\": y}\n",
    "\n",
    "#Esto se usaría una vez que la red neuronal ya está entrenada\n",
    "def clasificar(x, pesos):\n",
    "    # Corremos la red \"hacia adelante\"\n",
    "    resultados_feed_forward = ejecutar_adelante(x, pesos)\n",
    "    \n",
    "    # Buscamos la(s) clase(s) con scores mas altos (en caso de que haya mas de una con \n",
    "    # el mismo score estas podrian ser varias). Dado que se puede ejecutar en batch (x \n",
    "    # podria contener varios ejemplos), buscamos los maximos a lo largo del axis=1 \n",
    "    # (es decir, por filas)\n",
    "    max_scores = np.argmax(resultados_feed_forward[\"y\"], axis=1)\n",
    "\n",
    "    # Tomamos el primero de los maximos (podria usarse otro criterio, como ser eleccion aleatoria)\n",
    "    # Nuevamente, dado que max_scores puede contener varios renglones (uno por cada ejemplo),\n",
    "    # retornamos la primera columna\n",
    "    return max_scores#[:, 0]\n",
    "\n",
    "# x: n entradas para cada uno de los m ejemplos(nxm)\n",
    "# t: salida correcta (target) para cada uno de los m ejemplos (m x 1)\n",
    "# pesos: pesos (W y b)\n",
    "def train(x, t, pesos, learning_rate, epochs, cant_clases, cant_test):\n",
    "    # Cantidad de filas (i.e. cantidad de ejemplos)\n",
    "    m = np.size(x, 0) \n",
    "    \n",
    "    for i in range(epochs):\n",
    "        # Ejecucion de la red hacia adelante\n",
    "        resultados_feed_forward = ejecutar_adelante(x, pesos)\n",
    "        y = resultados_feed_forward[\"y\"]\n",
    "        h = resultados_feed_forward[\"h\"]\n",
    "        z = resultados_feed_forward[\"z\"]\n",
    "\n",
    "        # LOSS\n",
    "        # a. Exponencial de todos los scores\n",
    "        exp_scores = np.exp(y)\n",
    "\n",
    "        # b. Suma de todos los exponenciales de los scores, fila por fila (ejemplo por ejemplo).\n",
    "        #    Mantenemos las dimensiones (indicamos a NumPy que mantenga la segunda dimension del\n",
    "        #    arreglo, aunque sea una sola columna, para permitir el broadcast correcto en operaciones\n",
    "        #    subsiguientes)\n",
    "        sum_exp_scores = np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # c. \"Probabilidades\": normalizacion de las exponenciales del score de cada clase (dividiendo por \n",
    "        #    la suma de exponenciales de todos los scores), fila por fila\n",
    "        p = exp_scores / sum_exp_scores\n",
    "\n",
    "        # d. Calculo de la funcion de perdida global. Solo se usa la probabilidad de la clase correcta, \n",
    "        #    que tomamos del array t (\"target\")\n",
    "        loss = (1 / m) * np.sum( -np.log( p[range(m), t] ))\n",
    "\n",
    "        # Mostramos solo cada 1000 epochs\n",
    "        if i %1000 == 0:\n",
    "            print(\"Loss epoch\", i, \":\", loss)\n",
    "\n",
    "        # Extraemos los pesos a variables locales\n",
    "        w1 = pesos[\"w1\"]\n",
    "        b1 = pesos[\"b1\"]\n",
    "        w2 = pesos[\"w2\"]\n",
    "        b2 = pesos[\"b2\"]\n",
    "\n",
    "        # Ajustamos los pesos: Backpropagation\n",
    "        dL_dy = p                # Para todas las salidas, L' = p (la probabilidad)...\n",
    "        dL_dy[range(m), t] -= 1  # ... excepto para la clase correcta\n",
    "        dL_dy /= m\n",
    "\n",
    "        dL_dw2 = h.T.dot(dL_dy)                         # Ajuste para w2\n",
    "        dL_db2 = np.sum(dL_dy, axis=0, keepdims=True)   # Ajuste para b2\n",
    "\n",
    "        dL_dh = dL_dy.dot(w2.T)\n",
    "        \n",
    "        dL_dz = dL_dh       # El calculo dL/dz = dL/dh * dh/dz. La funcion \"h\" es la funcion de activacion de la capa oculta,\n",
    "        dL_dz[z <= 0] = 0   # para la que usamos ReLU. La derivada de la funcion ReLU: 1(z > 0) (0 en otro caso)\n",
    "\n",
    "        dL_dw1 = x.T.dot(dL_dz)                         # Ajuste para w1\n",
    "        dL_db1 = np.sum(dL_dz, axis=0, keepdims=True)   # Ajuste para b1\n",
    "\n",
    "        # Aplicamos el ajuste a los pesos\n",
    "        w1 += -learning_rate * dL_dw1\n",
    "        b1 += -learning_rate * dL_db1\n",
    "        w2 += -learning_rate * dL_dw2\n",
    "        b2 += -learning_rate * dL_db2\n",
    "\n",
    "        # Actualizamos la estructura de pesos\n",
    "        # Extraemos los pesos a variables locales\n",
    "        pesos[\"w1\"] = w1\n",
    "        pesos[\"b1\"] = b1\n",
    "        pesos[\"w2\"] = w2\n",
    "        pesos[\"b2\"] = b2\n",
    "    \n",
    "    '''APLICACIÓN EJERCICIO 2'''\n",
    "    '''Ejercicio 2_a'''\n",
    "    precision_2(x=x, t=t, pesos=pesos, conjunto='train')\n",
    "\n",
    "    '''Ejercicio 2_b'''\n",
    "    x_test, t_test = generar_datos_clasificacion(\n",
    "        cantidad_ejemplos=cant_test, \n",
    "        cantidad_clases=cant_clases)\n",
    "    precision_2(x=x_test, t=t_test, pesos=pesos, conjunto='test')\n",
    "\n",
    "\n",
    "def iniciar(numero_clases, numero_ejemplos, numero_test,graficar_datos):\n",
    "    # Generamos datos\n",
    "    x, t = generar_datos_clasificacion(numero_ejemplos, numero_clases)\n",
    "\n",
    "    # Graficamos los datos si es necesario\n",
    "    if graficar_datos:\n",
    "        # Parametro: \"c\": color (un color distinto para cada clase en t)\n",
    "        plt.scatter(x[:, 0], x[:, 1], c=t)\n",
    "        plt.show()\n",
    "\n",
    "    # Inicializa pesos de la red\n",
    "    NEURONAS_CAPA_OCULTA = 100\n",
    "    NEURONAS_ENTRADA = 2\n",
    "    pesos = inicializar_pesos(n_entrada=NEURONAS_ENTRADA, n_capa_2=NEURONAS_CAPA_OCULTA, n_capa_3=numero_clases)\n",
    "\n",
    "    # Entrena\n",
    "    LEARNING_RATE=1\n",
    "    EPOCHS=10000\n",
    "    train(x, t, pesos, LEARNING_RATE, EPOCHS, cant_clases=numero_clases, cant_test=numero_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "Modificar el programa para que:\n",
    "\n",
    "> a. Mida la precisión de clasificación (accuracy) además del valor de Loss\n",
    "\n",
    "> b. Utilice un conjunto de test independiente para realizar dicha medición (en lugar de utilizar\n",
    "los mismos datos de entrenamiento). Este punto requiere generar más ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_2(x, t, pesos, conjunto):\n",
    "    resultados = clasificar(x,pesos)\n",
    "    #Equal genera un vector booleano donde indica si los elementos de ambos vectores son iguales (True) o diferentes (False)\n",
    "    correctas = np.equal(t, resultados)\n",
    "    #mean puede calcular la media incluso para vectores booleanos\n",
    "    precision = np.mean(correctas)\n",
    "    print(f\"Precision conjunto {conjunto}: {precision*100:0.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0 : 1.11997237644542\n",
      "Loss epoch 1000 : 0.20514623778521213\n",
      "Loss epoch 2000 : 0.19530422309151715\n",
      "Loss epoch 3000 : 0.1870399095223022\n",
      "Loss epoch 4000 : 0.16782269453297816\n",
      "Loss epoch 5000 : 0.14910940055355587\n",
      "Loss epoch 6000 : 0.13521495063713387\n",
      "Loss epoch 7000 : 0.13608581515942247\n",
      "Loss epoch 8000 : 0.12611748195700107\n",
      "Loss epoch 9000 : 0.11148332085056238\n",
      "Precision conjunto train: 94.7 %\n",
      "Precision conjunto test: 96.8 %\n"
     ]
    }
   ],
   "source": [
    "iniciar(numero_clases=3, numero_ejemplos=300, numero_test=500,graficar_datos=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
